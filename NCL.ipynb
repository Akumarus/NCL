{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, preprocessing, model_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from activation import fun_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Neural Network.\n",
    "    \"\"\"\n",
    "    dim: int  # Dimension of the data (features)\n",
    "    t: int  # Dimension of the target (labels)\n",
    "    max_iter: int  # Number of iterations\n",
    "    neurons: int  # Number of neurons in the input layer\n",
    "    learning_rate: float  # Step\n",
    "    # Matrix\n",
    "    input_weight: np.array\n",
    "    output_weight: np.array\n",
    "    bias_input_layer: np.array\n",
    "    bias_output_layer: np.array\n",
    "    temp_h: np.array  # Value of the hidden layer before applying activation.\n",
    "    temp_o: np.array  # Value of the output layer before applying activation.\n",
    "    # Neuronal functions\n",
    "    activation = None\n",
    "    activation_der = None\n",
    "\n",
    "    def __init__(self, seed=None):\n",
    "        \"\"\"\n",
    "        Fix the random number generator.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def get_layers(self):\n",
    "        \"\"\"\n",
    "        Feed forward random assignation of the two layers.\n",
    "        \"\"\"\n",
    "        self.get_input_layer()\n",
    "        self.get_output_layer()\n",
    "\n",
    "    def get_input_layer(self):\n",
    "        \"\"\"\n",
    "        Weights and bias for the hidden layer.\n",
    "        \"\"\"\n",
    "        self.input_weight = np.random.random((self.dim,\n",
    "                                              self.neurons)) * 2.0 - 1.0\n",
    "        # self.bias_input_layer = np.random.random((self.neurons, 1))\n",
    "        self.bias_input_layer = np.zeros((self.neurons, 1))\n",
    "\n",
    "    def get_output_layer(self):\n",
    "        \"\"\"\n",
    "        Weight and bias for the output layer.\n",
    "        \"\"\"\n",
    "        self.output_weight = np.random.random((self.neurons, 1)) * 2.0 - 1.0\n",
    "        # self.bias_output_layer = np.random.random((self.t, 1))\n",
    "        self.bias_output_layer = np.zeros((self.t, 1))\n",
    "    def initial(self, x, y,\n",
    "                max_iter,\n",
    "                neurons,\n",
    "                learning_rate,\n",
    "                neuronal_fun):\n",
    "        \"\"\"\n",
    "        Initialize the neural network layers.\n",
    "        :param x: numpy.array with data (intances and features).\n",
    "        :param y: numpy.array with the target to predict.\n",
    "        :param int max_iter: number of iterations for training.\n",
    "        :param int neurons: number of neurons in the hidden layer.\n",
    "        :param float learning_rate: step to add in each iteration.\n",
    "        :param str neuronal_fun: function for activation functions in \n",
    "        \"\"\"\n",
    "        self.dim = x.shape[1]\n",
    "        self.t = y.shape[1]\n",
    "        self.max_iter = max_iter\n",
    "        self.neurons = neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = fun_dict[neuronal_fun]['activation']\n",
    "        self.activation_der = fun_dict[neuronal_fun]['derivative']\n",
    "        self.get_layers()\n",
    "        return self\n",
    "\n",
    "    def train(self, x, y,\n",
    "              max_iter: int = 1000,\n",
    "              neurons: int = 10,\n",
    "              learning_rate: float = 1.0,\n",
    "              neuronal_fun='sigmoid'):\n",
    "        \"\"\"\n",
    "        Train the neural network with gradient descent.\n",
    "        :param x: numpy.array with data (intances and features).\n",
    "        :param y: numpy.array with the target to predict.\n",
    "        :param int max_iter: number of iterations for training.\n",
    "        :param int neurons: number of neurons in the hidden layer.\n",
    "        :param float learning_rate: step to add in each iteration.\n",
    "        :param str neuronal_fun: function for activation functions in \n",
    "        \"\"\"\n",
    "        self.initial(x=x,\n",
    "                     y=y,\n",
    "                     max_iter=max_iter,\n",
    "                     neurons=neurons,\n",
    "                     learning_rate=learning_rate,\n",
    "                     neuronal_fun=neuronal_fun)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            # print('Iteration =', iteration)\n",
    "            self.backward(x, y)\n",
    "\n",
    "    def backward(self, x, y, penalty):\n",
    "        \"\"\"\n",
    "        Back propagation formula with a penalty.\n",
    "        :param x:\n",
    "        :param y:\n",
    "        :param penalty:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        hidden_layer, output_layer = self.forward(x)\n",
    "        error = output_layer - y\n",
    "        # print('Error =', np.linalg.norm(error), ', NC penalty =', np.linalg.norm(penalty))\n",
    "        nc_error = error + penalty\n",
    "\n",
    "        # Output layer\n",
    "        output_delta = nc_error * self.activation_der(self.temp_o)\n",
    "        # print('Norm of the gradient of output layer =', np.linalg.norm(output_delta))\n",
    "        self.bias_output_layer -= np.mean(self.learning_rate * output_delta)\n",
    "        self.output_weight -= self.learning_rate * np.dot(hidden_layer.T, output_delta)\n",
    "\n",
    "        # Hidden layer\n",
    "        hidden_delta = np.dot(output_delta, self.output_weight.T) * self.activation_der(self.temp_h)\n",
    "        # print('Norm of the gradient of hidden layer =', np.linalg.norm(hidden_delta))\n",
    "        self.bias_input_layer -= np.mean(self.learning_rate * hidden_delta, axis=0).reshape(self.neurons, 1)\n",
    "        self.input_weight -= self.learning_rate * np.dot(x.T, hidden_delta)\n",
    "\n",
    "    def forward(self, x_test):\n",
    "        self.temp_h = np.dot(x_test, self.input_weight) + self.bias_input_layer.T\n",
    "        hidden_layer = self.activation(self.temp_h)\n",
    "        self.temp_o = np.dot(hidden_layer, self.output_weight) + self.bias_output_layer.T\n",
    "        output_layer = self.activation(self.temp_o)\n",
    "        return hidden_layer, output_layer\n",
    "\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        _, output_layer = self.forward(x_test)\n",
    "        return output_layer\n",
    "\n",
    "class NCL:\n",
    "    \"\"\"\n",
    "    Negative Correlation Learning ensemble.\n",
    "    \"\"\"\n",
    "    size: int\n",
    "    max_iter: int\n",
    "    lambda_: float\n",
    "    learning_rate: float\n",
    "    base_learner = list\n",
    "    rmse_array: np.array\n",
    "\n",
    "    def train(self, x, y, size, neurons, max_iter, lambda_, learning_rate, neural_fun='sigmoid'):\n",
    "        \"\"\"\n",
    "        Training ensemble\n",
    "        :param x: data.\n",
    "        :param y: target.\n",
    "        :param size: number of base learners.\n",
    "        :param neurons:\n",
    "        :param max_iter:\n",
    "        :param lambda_:\n",
    "        :param learning_rate:\n",
    "        :param str neural_fun:\n",
    "        \"\"\"\n",
    "        # Parameter\n",
    "        self.size = size\n",
    "        self.max_iter = max_iter\n",
    "        self.lambda_ = lambda_\n",
    "        self.base_learner = [NeuralNetwork(seed=s).initial(x=x,\n",
    "                                                           y=y,\n",
    "                                                           neurons=neurons,\n",
    "                                                           learning_rate=learning_rate,\n",
    "                                                           neuronal_fun=neural_fun,\n",
    "                                                           max_iter=max_iter)\n",
    "                             for s in range(self.size)]\n",
    "        # Saving RMSE in training\n",
    "        self.rmse_array = np.inf * np.ones(self.max_iter)\n",
    "\n",
    "        # Training\n",
    "        for iteration in range(self.max_iter):  # Each epoch\n",
    "            # print('Iteration =', iteration)\n",
    "            f_bar = self.predict(x)\n",
    "            for s in range(self.size):  # Each base learner\n",
    "                penalty = - self.lambda_ * (self.base_learner[s].predict(x) - f_bar)\n",
    "                self.base_learner[s].backward(x, y, penalty)\n",
    "                # print()\n",
    "            # print()\n",
    "            self.rmse_array[iteration] = rmse(f_bar, y)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :return: f_bar\n",
    "        \"\"\"\n",
    "        f_bar = np.mean([self.base_learner[s].predict(x) for s in range(self.size)],\n",
    "                        axis=0)\n",
    "        return f_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train =  90\n",
      "Test =  60\n",
      "Colums =  4\n",
      "Lambda =  1\n",
      "RMSE = 1.2861488997497166\n",
      "Время обучения = 1.746004599990556\n",
      "Время отклика = 0.0004096000047866255\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    ''' Загрузка датасета и предобработка данных '''\n",
    "\n",
    "    x, y = datasets.load_iris(return_X_y=True)\n",
    "    x = preprocessing.scale(x)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    return x, y\n",
    "\n",
    "def rmse(a, b):\n",
    "\n",
    "    ''' Расчет среднеквадратичного отклонения '''\n",
    "\n",
    "    return sqrt(mean_squared_error(a, b)) \n",
    "\n",
    "def test():\n",
    "\n",
    "    # Загрузка датасета и формирование выборок\n",
    "    x, y = load_dataset()\n",
    "    x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size= 0.4, random_state=0)\n",
    "    print('Train = ',  x_train.shape[0])\n",
    "    print('Test = ',   x_test.shape[0])\n",
    "    print('Colums = ', x_train.shape[1])\n",
    "\n",
    "    # Параметры обучения\n",
    "    max_iter = 500\n",
    "    size = 10\n",
    "    h = 15                  # Количество нейронов в скрытом слое\n",
    "    lambda_ = 1\n",
    "    learning_rate = 0.001   # Скорость обучения\n",
    "    print('Lambda = ', lambda_)\n",
    "    \n",
    "    # Создание ансамбля моделей\n",
    "    model = NCL()\n",
    "\n",
    "    # Обучение\n",
    "    train_time_start = time.perf_counter()\n",
    "    model.train(x_train, y_train, size, h, max_iter, lambda_, learning_rate, neural_fun='leaky_relu')\n",
    "    train_time_end = time.perf_counter()\n",
    "\n",
    "    # Тестирование\n",
    "    test_time_start = time.perf_counter()\n",
    "    pred = model.predict(x_test)\n",
    "    test_time_end = time.perf_counter()\n",
    "\n",
    "    # RMSE\n",
    "    rmse_value = rmse(pred, y_test)\n",
    "    print('RMSE =', rmse_value)\n",
    "\n",
    "    # Время обучения\n",
    "    print('Время обучения =', train_time_end - train_time_start)\n",
    "    print('Время отклика =', test_time_end - test_time_start)\n",
    "\n",
    "    # Кривая обучения \n",
    "    plt.plot(model.rmse_array)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Neural Network.\n",
    "    \"\"\"\n",
    "    dim: int  # Dimension of the data (features)\n",
    "    t: int  # Dimension of the target (labels)\n",
    "    max_iter: int  # Number of iterations\n",
    "    neurons: int  # Number of neurons in the input layer\n",
    "    learning_rate: float  # Step\n",
    "    # Matrix\n",
    "    input_weight: np.array\n",
    "    output_weight: np.array\n",
    "    bias_input_layer: np.array\n",
    "    bias_output_layer: np.array\n",
    "    temp_h: np.array  # Value of the hidden layer before applying activation.\n",
    "    temp_o: np.array  # Value of the output layer before applying activation.\n",
    "    # Neuronal functions\n",
    "    activation = None\n",
    "    activation_der = None\n",
    "\n",
    "    def __init__(self, seed=None):\n",
    "        \"\"\"\n",
    "        Fix the random number generator.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def get_layers(self):\n",
    "        \"\"\"\n",
    "        Feed forward random assignation of the two layers.\n",
    "        \"\"\"\n",
    "        self.get_input_layer()\n",
    "        self.get_output_layer()\n",
    "\n",
    "    def get_input_layer(self):\n",
    "        \"\"\"\n",
    "        Weights and bias for the hidden layer.\n",
    "        \"\"\"\n",
    "        self.input_weight = np.random.random((self.dim,\n",
    "                                              self.neurons)) * 2.0 - 1.0\n",
    "        # self.bias_input_layer = np.random.random((self.neurons, 1))\n",
    "        self.bias_input_layer = np.zeros((self.neurons, 1))\n",
    "\n",
    "    def get_output_layer(self):\n",
    "        \"\"\"\n",
    "        Weight and bias for the output layer.\n",
    "        \"\"\"\n",
    "        self.output_weight = np.random.random((self.neurons, 1)) * 2.0 - 1.0\n",
    "        # self.bias_output_layer = np.random.random((self.t, 1))\n",
    "        self.bias_output_layer = np.zeros((self.t, 1))\n",
    "    def initial(self, x, y,\n",
    "                max_iter,\n",
    "                neurons,\n",
    "                learning_rate,\n",
    "                neuronal_fun):\n",
    "        \"\"\"\n",
    "        Initialize the neural network layers.\n",
    "        :param x: numpy.array with data (intances and features).\n",
    "        :param y: numpy.array with the target to predict.\n",
    "        :param int max_iter: number of iterations for training.\n",
    "        :param int neurons: number of neurons in the hidden layer.\n",
    "        :param float learning_rate: step to add in each iteration.\n",
    "        :param str neuronal_fun: function for activation functions in \n",
    "        \"\"\"\n",
    "        self.dim = x.shape[1]\n",
    "        self.t = y.shape[1]\n",
    "        self.max_iter = max_iter\n",
    "        self.neurons = neurons\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = fun_dict[neuronal_fun]['activation']\n",
    "        self.activation_der = fun_dict[neuronal_fun]['derivative']\n",
    "        self.get_layers()\n",
    "        return self\n",
    "\n",
    "    def train(self, x, y,\n",
    "              max_iter: int = 1000,\n",
    "              neurons: int = 10,\n",
    "              learning_rate: float = 1.0,\n",
    "              neuronal_fun='sigmoid'):\n",
    "        \"\"\"\n",
    "        Train the neural network with gradient descent.\n",
    "        :param x: numpy.array with data (intances and features).\n",
    "        :param y: numpy.array with the target to predict.\n",
    "        :param int max_iter: number of iterations for training.\n",
    "        :param int neurons: number of neurons in the hidden layer.\n",
    "        :param float learning_rate: step to add in each iteration.\n",
    "        :param str neuronal_fun: function for activation functions in \n",
    "        \"\"\"\n",
    "        self.initial(x=x,\n",
    "                     y=y,\n",
    "                     max_iter=max_iter,\n",
    "                     neurons=neurons,\n",
    "                     learning_rate=learning_rate,\n",
    "                     neuronal_fun=neuronal_fun)\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            # print('Iteration =', iteration)\n",
    "            self.backward(x, y)\n",
    "\n",
    "    def backward(self, x, y, penalty):\n",
    "        \"\"\"\n",
    "        Back propagation formula with a penalty.\n",
    "        :param x:\n",
    "        :param y:\n",
    "        :param penalty:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        hidden_layer, output_layer = self.forward(x)\n",
    "        error = output_layer - y\n",
    "        # print('Error =', np.linalg.norm(error), ', NC penalty =', np.linalg.norm(penalty))\n",
    "        nc_error = error + penalty\n",
    "\n",
    "        # Output layer\n",
    "        output_delta = nc_error * self.activation_der(self.temp_o)\n",
    "        # print('Norm of the gradient of output layer =', np.linalg.norm(output_delta))\n",
    "        self.bias_output_layer -= np.mean(self.learning_rate * output_delta)\n",
    "        self.output_weight -= self.learning_rate * np.dot(hidden_layer.T, output_delta)\n",
    "\n",
    "        # Hidden layer\n",
    "        hidden_delta = np.dot(output_delta, self.output_weight.T) * self.activation_der(self.temp_h)\n",
    "        # print('Norm of the gradient of hidden layer =', np.linalg.norm(hidden_delta))\n",
    "        self.bias_input_layer -= np.mean(self.learning_rate * hidden_delta, axis=0).reshape(self.neurons, 1)\n",
    "        self.input_weight -= self.learning_rate * np.dot(x.T, hidden_delta)\n",
    "\n",
    "    def forward(self, x_test):\n",
    "        self.temp_h = np.dot(x_test, self.input_weight) + self.bias_input_layer.T\n",
    "        hidden_layer = self.activation(self.temp_h)\n",
    "        self.temp_o = np.dot(hidden_layer, self.output_weight) + self.bias_output_layer.T\n",
    "        output_layer = self.activation(self.temp_o)\n",
    "        return hidden_layer, output_layer\n",
    "\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        _, output_layer = self.forward(x_test)\n",
    "        return output_layer\n",
    "\n",
    "class NCL:\n",
    "    \"\"\"\n",
    "    Negative Correlation Learning ensemble.\n",
    "    \"\"\"\n",
    "    size: int\n",
    "    max_iter: int\n",
    "    lambda_: float\n",
    "    learning_rate: float\n",
    "    base_learner = list\n",
    "    rmse_array: np.array\n",
    "\n",
    "    def train(self, x, y, size, neurons, max_iter, lambda_, learning_rate, neural_fun='sigmoid'):\n",
    "        \"\"\"\n",
    "        Training ensemble\n",
    "        :param x: data.\n",
    "        :param y: target.\n",
    "        :param size: number of base learners.\n",
    "        :param neurons:\n",
    "        :param max_iter:\n",
    "        :param lambda_:\n",
    "        :param learning_rate:\n",
    "        :param str neural_fun:\n",
    "        \"\"\"\n",
    "        # Parameter\n",
    "        self.size = size\n",
    "        self.max_iter = max_iter\n",
    "        self.lambda_ = lambda_\n",
    "        self.base_learner = [NeuralNetwork(seed=s).initial(x=x,\n",
    "                                                           y=y,\n",
    "                                                           neurons=neurons,\n",
    "                                                           learning_rate=learning_rate,\n",
    "                                                           neuronal_fun=neural_fun,\n",
    "                                                           max_iter=max_iter)\n",
    "                             for s in range(self.size)]\n",
    "        # Saving RMSE in training\n",
    "        self.rmse_array = np.inf * np.ones(self.max_iter)\n",
    "\n",
    "        # Training\n",
    "        for iteration in range(self.max_iter):  # Each epoch\n",
    "            # print('Iteration =', iteration)\n",
    "            f_bar = self.predict(x)\n",
    "            for s in range(self.size):  # Each base learner\n",
    "                penalty = - self.lambda_ * (self.base_learner[s].predict(x) - f_bar)\n",
    "                self.base_learner[s].backward(x, y, penalty)\n",
    "                # print()\n",
    "            # print()\n",
    "            self.rmse_array[iteration] = rmse(f_bar, y)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        :param x:\n",
    "        :return: f_bar\n",
    "        \"\"\"\n",
    "        f_bar = np.mean([self.base_learner[s].predict(x) for s in range(self.size)],\n",
    "                        axis=0)\n",
    "        return f_bar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
